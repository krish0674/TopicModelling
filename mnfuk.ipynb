{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7421484,"sourceType":"datasetVersion","datasetId":4317917}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import NMF\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom gensim.models import Word2Vec\nfrom gensim.corpora.dictionary import Dictionary\nfrom gensim.models.coherencemodel import CoherenceModel\nimport nltk\nnltk.download('punkt')\nnltk.download('wordnet')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-13T08:21:20.430719Z","iopub.execute_input":"2024-05-13T08:21:20.431094Z","iopub.status.idle":"2024-05-13T08:21:34.012882Z","shell.execute_reply.started":"2024-05-13T08:21:20.431055Z","shell.execute_reply":"2024-05-13T08:21:34.011724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\ndf = pd.read_csv(r\"/kaggle/input/kcctn-final-2/train_TamilNadu.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-13T09:59:42.120596Z","iopub.execute_input":"2024-05-13T09:59:42.120953Z","iopub.status.idle":"2024-05-13T09:59:44.670093Z","shell.execute_reply.started":"2024-05-13T09:59:42.120928Z","shell.execute_reply":"2024-05-13T09:59:44.669001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df[df['QueryType'].isin(['Government Schemes'])]","metadata":{"execution":{"iopub.status.busy":"2024-05-13T09:59:44.672116Z","iopub.execute_input":"2024-05-13T09:59:44.672433Z","iopub.status.idle":"2024-05-13T09:59:44.742477Z","shell.execute_reply.started":"2024-05-13T09:59:44.672408Z","shell.execute_reply":"2024-05-13T09:59:44.741234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\n# Drop NaN values\ndf = df.dropna(subset=['QueryText_processed'])\ndf['paper_text_processed'] = df['QueryText_processed'].map(lambda x: re.sub('[,\\.!?]', '', x).lower())\n# Print out the first rows of papers\ndf['paper_text_processed'].head()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-13T09:59:44.743814Z","iopub.execute_input":"2024-05-13T09:59:44.744187Z","iopub.status.idle":"2024-05-13T09:59:44.803920Z","shell.execute_reply.started":"2024-05-13T09:59:44.744155Z","shell.execute_reply":"2024-05-13T09:59:44.803009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gensim\nfrom gensim.utils import simple_preprocess\nimport nltk\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nstop_words.extend(['from', 'subject', 're', 'edu', 'use'])\ndef sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) \n             if word not in stop_words] for doc in texts]\n\ndata = df.paper_text_processed.values.tolist()\ndocuments=data\ndata_words = list(sent_to_words(data))\ndata_words = remove_stopwords(data_words)\nprint(data_words[:1][0][:30])","metadata":{"execution":{"iopub.status.busy":"2024-05-13T09:54:55.190109Z","iopub.execute_input":"2024-05-13T09:54:55.190483Z","iopub.status.idle":"2024-05-13T09:55:09.969166Z","shell.execute_reply.started":"2024-05-13T09:54:55.190455Z","shell.execute_reply":"2024-05-13T09:55:09.968108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gensim.corpora as corpora\nid2word = corpora.Dictionary(data_words)\ntexts = data_words\ncorpus = [id2word.doc2bow(text) for text in texts]\nprint(corpus[:1][0][:30])","metadata":{"execution":{"iopub.status.busy":"2024-05-13T05:33:02.578655Z","iopub.execute_input":"2024-05-13T05:33:02.579185Z","iopub.status.idle":"2024-05-13T05:33:09.217175Z","shell.execute_reply.started":"2024-05-13T05:33:02.579147Z","shell.execute_reply":"2024-05-13T05:33:09.216049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntf_idf_vect = TfidfVectorizer(min_df=50, stop_words='english')\nX = tf_idf_vect.fit_transform(documents)\n\nmodel = NMF(n_components=6, random_state=5)\n\nmodel.fit(X)\n\nnmf_features = model.transform(X)\n\nprint(f'Input features matrix shape - {X.shape}')\nprint(f'NMF features shape - {nmf_features.shape}')\n\ncomponents_df = pd.DataFrame(model.components_, columns=tf_idf_vect.get_feature_names_out())\n\nfor topic in range(components_df.shape[0]):\n    tmp = components_df.iloc[topic]\n    print(f'For topic {topic+1} the words with the highest value are:')\n    print(tmp.nlargest(10))\n    print('\\n')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-13T05:33:09.218829Z","iopub.execute_input":"2024-05-13T05:33:09.219132Z","iopub.status.idle":"2024-05-13T05:33:28.786168Z","shell.execute_reply.started":"2024-05-13T05:33:09.219107Z","shell.execute_reply":"2024-05-13T05:33:28.784557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"components_df = pd.DataFrame(model.components_, columns=tf_idf_vect.get_feature_names_out())\n\ntopics = components_df.apply(lambda row: [tf_idf_vect.get_feature_names_out()[i] for i in row.argsort()[:-11:-1]], axis=1).tolist()\n\n# Compute coherence score\ncoherence_model = CoherenceModel(topics=topics, texts=data_words, dictionary=id2word, coherence='c_v')\ncoherence_score = coherence_model.get_coherence()\n\nprint(\"Coherence Score:\", coherence_score)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T05:33:28.787354Z","iopub.execute_input":"2024-05-13T05:33:28.787658Z","iopub.status.idle":"2024-05-13T05:33:39.770607Z","shell.execute_reply.started":"2024-05-13T05:33:28.787632Z","shell.execute_reply":"2024-05-13T05:33:39.769395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"coherence_model = CoherenceModel(topics=topics, texts=data_words, dictionary=id2word, coherence='c_uci')\ncoherence_score = coherence_model.get_coherence()\n\nprint(\"Coherence Score:\", coherence_score)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T05:33:39.773258Z","iopub.execute_input":"2024-05-13T05:33:39.774175Z","iopub.status.idle":"2024-05-13T05:33:50.700752Z","shell.execute_reply.started":"2024-05-13T05:33:39.774142Z","shell.execute_reply":"2024-05-13T05:33:50.699394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"coherence_model = CoherenceModel(topics=topics, texts=data_words, dictionary=id2word, coherence='u_mass')\ncoherence_score = coherence_model.get_coherence()\n\nprint(\"Coherence Score:\", coherence_score)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T05:33:50.702707Z","iopub.execute_input":"2024-05-13T05:33:50.703130Z","iopub.status.idle":"2024-05-13T05:33:55.892382Z","shell.execute_reply.started":"2024-05-13T05:33:50.703089Z","shell.execute_reply":"2024-05-13T05:33:55.891020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ntopic_distribution = nmf_features.argmax(axis=1)  \n\ntopic_counts = pd.Series(topic_distribution).value_counts().sort_index()\n\nplt.figure(figsize=(10, 6))\ntopic_counts.plot(kind='bar', color='skyblue')\nplt.title('Number of Documents per Topic')\nplt.xlabel('Topic')\nplt.ylabel('Number of Documents')\nplt.xticks(rotation=45)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-13T05:33:55.893767Z","iopub.execute_input":"2024-05-13T05:33:55.894696Z","iopub.status.idle":"2024-05-13T05:33:56.287520Z","shell.execute_reply.started":"2024-05-13T05:33:55.894659Z","shell.execute_reply":"2024-05-13T05:33:56.286506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom itertools import cycle\n\n# Initialize a dictionary to store the topic counts for each year\nyearly_topic_counts = {}\nnum_topics = 6\n\n# Define the starting and ending years\nstart_year = 2009\nend_year = 2018\n\n# Initialize yearly topic counts for each year and each topic\nfor year in range(start_year, end_year + 1):\n    yearly_topic_counts[year] = {topic: 0 for topic in range(num_topics)}\n\n# Iterate over each example in the corpus\nfor index, (example, year) in enumerate(zip(corpus, df['Year'])):\n    year = int(year)  # Convert year to integer\n\n    # Check if the year is within our range\n    if start_year <= year <= end_year:\n        topic_distribution = lda_model[example]\n\n        # Find the dominant topic for the example (topic with maximum probability)\n        dominant_topic = max(topic_distribution, key=lambda x: x[1])[0]\n\n        # Update the topic counts dictionary for the corresponding year\n        yearly_topic_counts[year][dominant_topic] += 1\n\n# Plotting all topics over the years 2009-2018\n# Since there are 5 topics, you can choose how to split them into two graphs.\n\n# IDs for the topics can be adjusted as needed, here's a split example:\ntop_topics = [0, 1, 2]  # Topics for the first graph\nremaining_topics = [ 4,5,3]  # Topics for the second graph\n\n# First graph for top 3 topics\nplt.figure(figsize=(10, 6))\ncolor_palette = cycle(['blue', 'orange', 'green'])\nfor topic, color in zip(top_topics, color_palette):\n    topic_counts = [yearly_topic_counts[year][topic] for year in range(start_year, end_year + 1)]\n    plt.plot(range(start_year, end_year + 1), topic_counts, color=color, label=f'Topic {topic}')\n\nplt.xlabel('Year')\nplt.ylabel('Number of Examples')\nplt.title('Trends of Top 3 Topics (2009-2018)')\nplt.legend(title='Topic', loc='upper left')\nplt.grid(True)\nplt.show()\n\n# Second graph for remaining topics\nplt.figure(figsize=(10, 6))\ncolor_palette = cycle(['red', 'purple'])\nfor topic, color in zip(remaining_topics, color_palette):\n    topic_counts = [yearly_topic_counts[year][topic] for year in range(start_year, end_year + 1)]\n    plt.plot(range(start_year, end_year + 1), topic_counts, color=color, label=f'Topic {topic}')\n\nplt.xlabel('Year')\nplt.ylabel('Number of Examples')\nplt.title('Trends of Remaining Topics (2009-2018)')\nplt.legend(title='Topic', loc='upper left')\nplt.grid(True)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ntopic_correlation = components_df.corr()\n\n# Plot the heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(topic_correlation, annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title('Topic Correlation Heatmap')\nplt.xlabel('Topics')\nplt.ylabel('Topics')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-13T05:33:56.288985Z","iopub.execute_input":"2024-05-13T05:33:56.289289Z"},"trusted":true},"execution_count":null,"outputs":[]}]}